{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8dWOsmtL6fu1mxLizBToY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leakysam/Soccer-Scraper/blob/main/Soccer_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCSqkBQFS65t",
        "outputId": "597e0fb5-3da6-488a-c8b9-42a5a4d16d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully scraped and stored in 'output_data.xlsx'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to format date as needed in the URL\n",
        "def format_date_url(date):\n",
        "    return date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Specify the starting date\n",
        "start_date = datetime(2024, 1, 1)\n",
        "\n",
        "# Number of days to scrape\n",
        "num_days = 21\n",
        "\n",
        "# Create an empty list to store the extracted data\n",
        "all_data = []\n",
        "\n",
        "# Iterate over the specified number of days\n",
        "for day in range(num_days):\n",
        "    # Calculate the current date\n",
        "    current_date = start_date + timedelta(days=day)\n",
        "\n",
        "    # Construct the URL with the dynamic date\n",
        "    url = f\"https://www.forebet.com/en/football-predictions/under-over-25-goals/{format_date_url(current_date)}\"\n",
        "\n",
        "    # Set the user-agent header\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    # Send an HTTP request to the URL with the specified headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            # Parse the HTML content of the page\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract data from specific HTML elements\n",
        "            team_divs = soup.find_all('div', class_='tnms')\n",
        "            avg_goals_divs = soup.find_all('div', class_='avg_sc tabonly')\n",
        "\n",
        "            # Extract coefficients from both div and span structures\n",
        "            coef_values = []\n",
        "            coef_divs = soup.find_all('div', class_='bigOnly prmod')\n",
        "            for div in coef_divs:\n",
        "                span = div.find('span', class_='lscrsp')\n",
        "                coef = div.find('span', class_='lscrsp') or div.find('span')\n",
        "                match = re.search(r'\\d+\\.\\d+', coef.get_text(strip=True)) if coef else None\n",
        "                coef_values.append(match.group() if match else 'N/A')\n",
        "\n",
        "            # Extract score values from both div and span structures\n",
        "            score_values = []\n",
        "            score_divs = soup.find_all('div', class_='lscr_td')\n",
        "            for div in score_divs:\n",
        "                b = div.find('b', class_='l_scr')\n",
        "                score_text = b.get_text(strip=True) if b else 'N/A'\n",
        "                score_values.append(score_text)\n",
        "\n",
        "            # Create lists to store the extracted data\n",
        "            teams = [div.get_text(strip=True) for div in team_divs]\n",
        "            avg_goals = [div.get_text(strip=True) for div in avg_goals_divs]\n",
        "\n",
        "            # Determine the minimum length of all lists\n",
        "            min_length = min(len(teams), len(avg_goals), len(coef_values), len(score_values))\n",
        "\n",
        "            # Append the extracted data to the list\n",
        "            for i in range(min_length):\n",
        "                data = {\n",
        "                    'Date': format_date_url(current_date),\n",
        "                    'Team': teams[i],\n",
        "                    'Avg. Goals': avg_goals[i],\n",
        "                    'Coef. Value': coef_values[i],\n",
        "                    'Score': score_values[i]\n",
        "                }\n",
        "                all_data.append(data)\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error extracting data: {e}\")\n",
        "    else:\n",
        "        print(f\"Error: Unable to fetch data. Status code: {response.status_code}\")\n",
        "\n",
        "# Convert the list of dictionaries to a Pandas DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file = 'output_data.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Data has been successfully scraped and stored in '{output_file}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to format date as needed in the URL\n",
        "def format_date_url(date):\n",
        "    return date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Specify the starting date\n",
        "start_date = datetime(2024, 1, 1)\n",
        "\n",
        "# Number of days to scrape\n",
        "num_days = 21\n",
        "\n",
        "# Create an empty list to store the extracted data\n",
        "all_data = []\n",
        "\n",
        "# Iterate over the specified number of days\n",
        "for day in range(num_days):\n",
        "    # Calculate the current date\n",
        "    current_date = start_date + timedelta(days=day)\n",
        "\n",
        "    # Construct the URL with the dynamic date\n",
        "    url = f\"https://www.forebet.com/en/football-predictions/corners/{format_date_url(current_date)}\"\n",
        "\n",
        "    # Set the user-agent header\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    # Send an HTTP request to the URL with the specified headers\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            # Parse the HTML content of the page\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract data from specific HTML elements\n",
        "            team_divs = soup.find_all('div', class_='tnms')\n",
        "            avg_corners_divs = soup.find_all('div', class_='avg_sc tabonly')\n",
        "            corners_divs = soup.find_all('div', class_='lscr_td lResTdSmall')\n",
        "\n",
        "            # Extract Avg. Corners values\n",
        "            avg_corners_values = [div.get_text(strip=True) for div in avg_corners_divs]\n",
        "\n",
        "            # Extract Corners values\n",
        "            corners_values = []\n",
        "            for div in corners_divs:\n",
        "                b = div.find('b', class_='l_scr')\n",
        "                corners_text = b.get_text(strip=True) if b else 'N/A'\n",
        "                corners_values.append(corners_text)\n",
        "\n",
        "            # Create lists to store the extracted data\n",
        "            teams = [div.get_text(strip=True) for div in team_divs]\n",
        "\n",
        "            # Determine the minimum length of all lists\n",
        "            min_length = min(len(teams), len(avg_corners_values), len(corners_values))\n",
        "\n",
        "            # Append the extracted data to the list\n",
        "            for i in range(min_length):\n",
        "                data = {\n",
        "                    'Date': format_date_url(current_date),\n",
        "                    'Home Team': teams[i].split('vs.')[0].strip(),\n",
        "                    'Away Team': teams[i].split('vs.')[1].strip() if 'vs.' in teams[i] else 'N/A',\n",
        "                    'Avg. Corners': avg_corners_values[i],\n",
        "                    'Corners': corners_values[i]\n",
        "                }\n",
        "                all_data.append(data)\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error extracting data: {e}\")\n",
        "    else:\n",
        "        print(f\"Error: Unable to fetch data. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "# Convert the list of dictionaries to a Pandas DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Print the DataFrame to see if the data looks correct\n",
        "print(df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file = 'output_corners.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Data has been successfully scraped and stored in '{output_file}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNlQO8puTGAa",
        "outputId": "849cbe5c-fa3c-4aca-e4ee-351621b6f5db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date                                    Home Team Away Team  \\\n",
            "0    2024-01-01                           Home teamAway team       N/A   \n",
            "1    2024-01-01     WS WanderersMacarthur FC01/01/2024 07:00       N/A   \n",
            "2    2024-01-01  SunderlandPreston North End01/01/2024 13:30       N/A   \n",
            "3    2024-01-01    Swansea CityWest Bromwich01/01/2024 16:00       N/A   \n",
            "4    2024-01-01       Stoke CityIpswich Town01/01/2024 16:00       N/A   \n",
            "..          ...                                          ...       ...   \n",
            "428  2024-01-21             GD ChavesRio Ave21/01/2024 19:00       N/A   \n",
            "429  2024-01-21   Aris SalonicaOlympiacos FC21/01/2024 19:30       N/A   \n",
            "430  2024-01-21          US LecceJuventus FC21/01/2024 20:45       N/A   \n",
            "431  2024-01-21          Girona FCSevilla FC21/01/2024 21:00       N/A   \n",
            "432  2024-01-21      Elche CFReal Valladolid21/01/2024 21:00       N/A   \n",
            "\n",
            "     Avg. Corners Corners  \n",
            "0    Avg. corners       9  \n",
            "1           11.98       5  \n",
            "2           10.45       8  \n",
            "3           10.39      13  \n",
            "4            9.40       7  \n",
            "..            ...     ...  \n",
            "428          9.03       2  \n",
            "429          9.33      10  \n",
            "430         10.43       9  \n",
            "431          9.98       5  \n",
            "432          9.42      12  \n",
            "\n",
            "[433 rows x 5 columns]\n",
            "Data has been successfully scraped and stored in 'output_corners.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "katr5Lk9Tee7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}